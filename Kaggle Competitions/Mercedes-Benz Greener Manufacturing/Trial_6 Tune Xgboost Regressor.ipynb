{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.594e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.355e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.355e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.295e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.346e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.093e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.643e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.350e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=6.565e-03, previous alpha=6.236e-03, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.771e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.318e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.318e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.318e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.599e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.599e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.599e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.751e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.751e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.418e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.403e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.403e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.290e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.897e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.689e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.270e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.270e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.270e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.215e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 51 iterations, alpha=4.285e-03, previous alpha=4.196e-03, with an active set of 48 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.393e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.266e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.197e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.197e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.197e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.567e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.870e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.870e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.107e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.107e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.012e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.999e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.419e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.250e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.974e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.816e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.816e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=5.709e-03, previous alpha=5.523e-03, with an active set of 35 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.890e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.557e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.291e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.291e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.770e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.770e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.770e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.416e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.416e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=6.199e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.874e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.604e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=4.819e-03, previous alpha=4.600e-03, with an active set of 34 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.174e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.540e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.202e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.038e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.417e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.245e-03, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.153e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.031e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.031e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.993e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.854e-03, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.637e-03, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.619e-03, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.616e-03, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.616e-03, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 95 iterations, alpha=1.615e-03, previous alpha=1.607e-03, with an active set of 86 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.530e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.098e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.834e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.834e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.654e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.593e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.417e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.709e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.530e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.530e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.421e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.087e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=2.021e-03, previous alpha=2.010e-03, with an active set of 63 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.960e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.324e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.599e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.182e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.182e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.182e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.162e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.154e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.085e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.012e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.661e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.661e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.571e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.571e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.538e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.250e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.022e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.946e-03, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.874e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.874e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.782e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.782e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.654e-03, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.570e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.570e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.555e-03, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=1.484e-03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.441e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.431e-03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.354e-03, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.302e-03, with an active set of 108 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.254e-03, with an active set of 111 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.242e-03, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.122e-03, with an active set of 123 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=1.185e-03, previous alpha=1.116e-03, with an active set of 124 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "#usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train=train.drop('y', axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train,y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602913449733\n",
      "12.593720436096191\n"
     ]
    }
   ],
   "source": [
    "'''Train the xgb model then predict the test data'''\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, Y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(elapsed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602913449733\n",
      "12.762730121612549\n"
     ]
    }
   ],
   "source": [
    "'''Train the xgb model then predict the test data'''\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "xgb_params = { \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, Y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain,num_boost_round=1250)\n",
    "y_pred = model.predict(dtest)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(elapsed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.600859184893\n",
      "11.67066764831543\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "est = XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4, seed=27,silent=1,base_score=y_mean)\n",
    "est.fit(X_train, Y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "print(r2_score(Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "dtrain = xgb.DMatrix(train, y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored 0.56841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,Tune XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1Fix learning rate and number of estimators for tuning tree-based parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg,X_train, Y_train,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, Y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        #alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    #alg.fit(X_train, Y_train,eval_metric='rmse')\n",
    "    print(cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "modelfit(xgb1,X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54859, std: 0.05794, params: {'max_depth': 3, 'min_child_weight': 1}, mean: 0.54710, std: 0.05835, params: {'min_child_weight': 3, 'max_depth': 3}, mean: 0.54687, std: 0.05719, params: {'min_child_weight': 5, 'max_depth': 3}, mean: 0.53884, std: 0.05633, params: {'min_child_weight': 1, 'max_depth': 5}, mean: 0.53845, std: 0.05594, params: {'min_child_weight': 3, 'max_depth': 5}, mean: 0.53792, std: 0.05848, params: {'min_child_weight': 5, 'max_depth': 5}, mean: 0.52185, std: 0.05972, params: {'min_child_weight': 1, 'max_depth': 7}, mean: 0.52920, std: 0.05833, params: {'min_child_weight': 3, 'max_depth': 7}, mean: 0.53292, std: 0.06005, params: {'min_child_weight': 5, 'max_depth': 7}, mean: 0.50849, std: 0.06067, params: {'min_child_weight': 1, 'max_depth': 9}, mean: 0.51546, std: 0.05607, params: {'min_child_weight': 3, 'max_depth': 9}, mean: 0.52280, std: 0.06027, params: {'min_child_weight': 5, 'max_depth': 9}]\n",
      "{'max_depth': 3, 'min_child_weight': 1}\n",
      "0.548586521039\n",
      "0.638502668924\n",
      "31.34999990463257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.50991, std: 0.05923, params: {'min_child_weight': 0.3, 'max_depth': 1}, mean: 0.50991, std: 0.05923, params: {'max_depth': 1, 'min_child_weight': 0.5}, mean: 0.50991, std: 0.05923, params: {'min_child_weight': 0.7, 'max_depth': 1}, mean: 0.50991, std: 0.05923, params: {'min_child_weight': 1.0, 'max_depth': 1}, mean: 0.54967, std: 0.05788, params: {'min_child_weight': 0.3, 'max_depth': 2}, mean: 0.54967, std: 0.05788, params: {'min_child_weight': 0.5, 'max_depth': 2}, mean: 0.54967, std: 0.05788, params: {'min_child_weight': 0.7, 'max_depth': 2}, mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 1.0}, mean: 0.54859, std: 0.05794, params: {'max_depth': 3, 'min_child_weight': 0.3}, mean: 0.54859, std: 0.05794, params: {'max_depth': 3, 'min_child_weight': 0.5}, mean: 0.54859, std: 0.05794, params: {'max_depth': 3, 'min_child_weight': 0.7}, mean: 0.54859, std: 0.05794, params: {'max_depth': 3, 'min_child_weight': 1.0}]\n",
      "{'min_child_weight': 0.3, 'max_depth': 2}\n",
      "0.549668358206\n",
      "0.643034924665\n",
      "15.598000049591064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':[1,2,3],\n",
    " 'min_child_weight':[0.3,0.5,0.7,1.0]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54967, std: 0.05788, params: {'min_child_weight': 0.1, 'max_depth': 2}, mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 0.2}, mean: 0.54967, std: 0.05788, params: {'min_child_weight': 0.3, 'max_depth': 2}]\n",
      "{'min_child_weight': 0.1, 'max_depth': 2}\n",
      "0.549668358206\n",
      "0.643034924665\n",
      "6.009000062942505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':[2],\n",
    " 'min_child_weight':[0.1,0.2,0.3]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 0.03}, mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 0.05}, mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 0.07}, mean: 0.54967, std: 0.05788, params: {'max_depth': 2, 'min_child_weight': 0.1}]\n",
      "{'max_depth': 2, 'min_child_weight': 0.03}\n",
      "0.549668358206\n",
      "0.643034924665\n",
      "7.2790000438690186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':[2],\n",
    " 'min_child_weight':[0.03,0.05,0.07,0.1]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored 0.55681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2.3 Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54967, std: 0.05788, params: {'gamma': 0.0}, mean: 0.54967, std: 0.05788, params: {'gamma': 0.1}, mean: 0.54967, std: 0.05788, params: {'gamma': 0.2}, mean: 0.54967, std: 0.05788, params: {'gamma': 0.3}, mean: 0.54967, std: 0.05788, params: {'gamma': 0.4}]\n",
      "{'gamma': 0.0}\n",
      "0.549668358206\n",
      "0.643034924665\n",
      "8.653000116348267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=54, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-calibrate the number of boosting rounds for the updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def modelfit(alg,X_train, Y_train,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, Y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        #alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    #alg.fit(X_train, Y_train,eval_metric='rmse')\n",
    "    print(cvresult.shape[0])\n",
    "    \n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=2,\n",
    " min_child_weight=0.3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "\n",
    "modelfit(xgb1,X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54635, std: 0.05743, params: {'subsample': 0.6, 'colsample_bytree': 0.6}, mean: 0.54788, std: 0.05869, params: {'subsample': 0.7, 'colsample_bytree': 0.6}, mean: 0.55002, std: 0.05900, params: {'subsample': 0.8, 'colsample_bytree': 0.6}, mean: 0.55105, std: 0.06062, params: {'subsample': 0.9, 'colsample_bytree': 0.6}, mean: 0.54851, std: 0.05816, params: {'subsample': 0.6, 'colsample_bytree': 0.7}, mean: 0.54719, std: 0.05707, params: {'subsample': 0.7, 'colsample_bytree': 0.7}, mean: 0.54960, std: 0.05857, params: {'subsample': 0.8, 'colsample_bytree': 0.7}, mean: 0.55089, std: 0.06010, params: {'subsample': 0.9, 'colsample_bytree': 0.7}, mean: 0.54592, std: 0.05815, params: {'subsample': 0.6, 'colsample_bytree': 0.8}, mean: 0.54698, std: 0.05885, params: {'subsample': 0.7, 'colsample_bytree': 0.8}, mean: 0.54917, std: 0.05797, params: {'subsample': 0.8, 'colsample_bytree': 0.8}, mean: 0.54935, std: 0.06073, params: {'subsample': 0.9, 'colsample_bytree': 0.8}, mean: 0.54453, std: 0.05671, params: {'subsample': 0.6, 'colsample_bytree': 0.9}, mean: 0.54713, std: 0.05673, params: {'subsample': 0.7, 'colsample_bytree': 0.9}, mean: 0.54839, std: 0.05840, params: {'subsample': 0.8, 'colsample_bytree': 0.9}, mean: 0.55034, std: 0.05978, params: {'subsample': 0.9, 'colsample_bytree': 0.9}]\n",
      "{'subsample': 0.9, 'colsample_bytree': 0.6}\n",
      "0.551045982325\n",
      "0.646900613019\n",
      "18.64400005340576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=52, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.54735, std: 0.05982, params: {'subsample': 0.75, 'colsample_bytree': 0.5}, mean: 0.54881, std: 0.05954, params: {'subsample': 0.8, 'colsample_bytree': 0.5}, mean: 0.55025, std: 0.06048, params: {'subsample': 0.85, 'colsample_bytree': 0.5}, mean: 0.55189, std: 0.06088, params: {'subsample': 0.9, 'colsample_bytree': 0.5}, mean: 0.55111, std: 0.06056, params: {'subsample': 0.95, 'colsample_bytree': 0.5}, mean: 0.54826, std: 0.05787, params: {'subsample': 0.75, 'colsample_bytree': 0.55}, mean: 0.54924, std: 0.05841, params: {'subsample': 0.8, 'colsample_bytree': 0.55}, mean: 0.54863, std: 0.05920, params: {'subsample': 0.85, 'colsample_bytree': 0.55}, mean: 0.55045, std: 0.05951, params: {'subsample': 0.9, 'colsample_bytree': 0.55}, mean: 0.55154, std: 0.05942, params: {'subsample': 0.95, 'colsample_bytree': 0.55}, mean: 0.54895, std: 0.05860, params: {'subsample': 0.75, 'colsample_bytree': 0.6}, mean: 0.55002, std: 0.05900, params: {'subsample': 0.8, 'colsample_bytree': 0.6}, mean: 0.54979, std: 0.05991, params: {'subsample': 0.85, 'colsample_bytree': 0.6}, mean: 0.55105, std: 0.06062, params: {'subsample': 0.9, 'colsample_bytree': 0.6}, mean: 0.55247, std: 0.05901, params: {'subsample': 0.95, 'colsample_bytree': 0.6}, mean: 0.54923, std: 0.05889, params: {'subsample': 0.75, 'colsample_bytree': 0.65}, mean: 0.54926, std: 0.05830, params: {'subsample': 0.8, 'colsample_bytree': 0.65}, mean: 0.55066, std: 0.05928, params: {'subsample': 0.85, 'colsample_bytree': 0.65}, mean: 0.55105, std: 0.05946, params: {'subsample': 0.9, 'colsample_bytree': 0.65}, mean: 0.55234, std: 0.05939, params: {'subsample': 0.95, 'colsample_bytree': 0.65}, mean: 0.54922, std: 0.05708, params: {'subsample': 0.75, 'colsample_bytree': 0.7}, mean: 0.54960, std: 0.05857, params: {'subsample': 0.8, 'colsample_bytree': 0.7}, mean: 0.55133, std: 0.05876, params: {'subsample': 0.85, 'colsample_bytree': 0.7}, mean: 0.55089, std: 0.06010, params: {'subsample': 0.9, 'colsample_bytree': 0.7}, mean: 0.55183, std: 0.05996, params: {'subsample': 0.95, 'colsample_bytree': 0.7}, mean: 0.54970, std: 0.05856, params: {'subsample': 0.75, 'colsample_bytree': 0.75}, mean: 0.55140, std: 0.05854, params: {'subsample': 0.8, 'colsample_bytree': 0.75}, mean: 0.55048, std: 0.06055, params: {'subsample': 0.85, 'colsample_bytree': 0.75}, mean: 0.55059, std: 0.05971, params: {'subsample': 0.9, 'colsample_bytree': 0.75}, mean: 0.55053, std: 0.05976, params: {'subsample': 0.95, 'colsample_bytree': 0.75}]\n",
      "{'subsample': 0.95, 'colsample_bytree': 0.6}\n",
      "0.552471502493\n",
      "0.646306543341\n",
      "28.85599994659424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'subsample':[i/100.0 for i in range(75,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(50,80,5)]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=52, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Tune Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.55247, std: 0.05901, params: {'reg_alpha': 1e-05}, mean: 0.55243, std: 0.05901, params: {'reg_alpha': 0.01}, mean: 0.55238, std: 0.05943, params: {'reg_alpha': 0.1}, mean: 0.55117, std: 0.05885, params: {'reg_alpha': 1}, mean: 0.54737, std: 0.06257, params: {'reg_alpha': 100}]\n",
      "{'reg_alpha': 1e-05}\n",
      "0.552471499624\n",
      "0.646306543341\n",
      "6.9019999504089355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=52, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.95, colsample_bytree=0.6,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.001}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.005}, mean: 0.55243, std: 0.05901, params: {'reg_alpha': 0.01}, mean: 0.55243, std: 0.05934, params: {'reg_alpha': 0.05}]\n",
      "{'reg_alpha': 0.005}\n",
      "0.552471540294\n",
      "0.646306851348\n",
      "7.101999998092651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    "'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]}\n",
    "\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=52, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.95, colsample_bytree=0.6,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.006}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.007}, mean: 0.55243, std: 0.05901, params: {'reg_alpha': 0.008}, mean: 0.55243, std: 0.05901, params: {'reg_alpha': 0.009}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.005}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.001}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.002}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.003}, mean: 0.55247, std: 0.05901, params: {'reg_alpha': 0.004}]\n",
      "{'reg_alpha': 0.005}\n",
      "0.552471540294\n",
      "0.646306851348\n",
      "10.83400011062622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    "'reg_alpha':[0.006,0.007,0.008,0.009, 0.005, 0.001, 0.002,0.003,0.004]}\n",
    "\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.1, n_estimators=52, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.95, colsample_bytree=0.6,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2.6 Reducing Learning Rate re-calibrate the number of boosting rounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def modelfit(alg,X_train, Y_train,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, Y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        #alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    #alg.fit(X_train, Y_train,eval_metric='rmse')\n",
    "    print(cvresult.shape[0])\n",
    "    \n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.0045, n_estimators=5000, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.95, colsample_bytree=0.6,reg_alpha=0.005,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "\n",
    "modelfit(xgb1,X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646219263052\n"
     ]
    }
   ],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1372, max_depth=2,\n",
    " min_child_weight=0.3, gamma=0, subsample=0.95, colsample_bytree=0.8,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "y_pred1 = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred1))\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored 0.55591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4, seed=27,silent=1,base_score=y_mean)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.56794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4, seed=27,silent=1)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.56481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4, seed=27,silent=1,base_score=y_mean)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.56686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.55179, std: 0.05914, params: {'subsample': 0.91, 'max_depth': 2}, mean: 0.55150, std: 0.05928, params: {'subsample': 0.92, 'max_depth': 2}, mean: 0.55161, std: 0.05928, params: {'subsample': 0.93, 'max_depth': 2}, mean: 0.55159, std: 0.05919, params: {'subsample': 0.94, 'max_depth': 2}, mean: 0.55172, std: 0.05943, params: {'subsample': 0.95, 'max_depth': 2}, mean: 0.55191, std: 0.05943, params: {'subsample': 0.96, 'max_depth': 2}, mean: 0.55205, std: 0.05945, params: {'subsample': 0.97, 'max_depth': 2}, mean: 0.54774, std: 0.05946, params: {'subsample': 0.91, 'max_depth': 3}, mean: 0.54732, std: 0.05999, params: {'subsample': 0.92, 'max_depth': 3}, mean: 0.54807, std: 0.06006, params: {'subsample': 0.93, 'max_depth': 3}, mean: 0.54832, std: 0.05998, params: {'subsample': 0.94, 'max_depth': 3}, mean: 0.54867, std: 0.06010, params: {'subsample': 0.95, 'max_depth': 3}, mean: 0.54887, std: 0.06021, params: {'subsample': 0.96, 'max_depth': 3}, mean: 0.54912, std: 0.06003, params: {'subsample': 0.97, 'max_depth': 3}, mean: 0.54373, std: 0.05968, params: {'subsample': 0.91, 'max_depth': 4}, mean: 0.54335, std: 0.05909, params: {'subsample': 0.92, 'max_depth': 4}, mean: 0.54418, std: 0.05938, params: {'subsample': 0.93, 'max_depth': 4}, mean: 0.54425, std: 0.05996, params: {'subsample': 0.94, 'max_depth': 4}, mean: 0.54332, std: 0.05972, params: {'subsample': 0.95, 'max_depth': 4}, mean: 0.54406, std: 0.05972, params: {'subsample': 0.96, 'max_depth': 4}, mean: 0.54429, std: 0.05987, params: {'subsample': 0.97, 'max_depth': 4}, mean: 0.53803, std: 0.05963, params: {'subsample': 0.91, 'max_depth': 5}, mean: 0.53762, std: 0.05904, params: {'subsample': 0.92, 'max_depth': 5}, mean: 0.53706, std: 0.05999, params: {'subsample': 0.93, 'max_depth': 5}, mean: 0.53771, std: 0.05980, params: {'subsample': 0.94, 'max_depth': 5}, mean: 0.53646, std: 0.05938, params: {'subsample': 0.95, 'max_depth': 5}, mean: 0.53667, std: 0.05933, params: {'subsample': 0.96, 'max_depth': 5}, mean: 0.53626, std: 0.05997, params: {'subsample': 0.97, 'max_depth': 5}]\n",
      "{'subsample': 0.97, 'max_depth': 2}\n",
      "0.552052244144\n",
      "0.646219263052\n",
      "1077.0039999485016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "param_test1 = {\n",
    "'max_depth':[2,3,4,5],\n",
    "#'subsample':[0.91,0.92,0.93,0.94,0.95,0.96,0.97]\n",
    "'subsample':[i/100.0 for i in [91,92,93,94,95,96,97]]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=2,\n",
    " subsample=0.95,\n",
    " objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.grid_scores_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#grid = grid.fit(X_train, Y_train)\n",
    "model=grid.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score( Y_test,y_pred))\n",
    "\n",
    "print(time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=2,\n",
    " subsample=0.97, objective= 'reg:linear', nthread=4, seed=27,silent=1)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.55470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4, seed=27,silent=1)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.56481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "estimator =XGBRegressor( learning_rate =0.0045, n_estimators=1250, max_depth=4,\n",
    " subsample=0.93, objective= 'reg:linear', nthread=4,base_score= y_mean, seed=27,silent=1)\n",
    "\n",
    "\n",
    "estimator.fit(train, y_train)\n",
    "y_pred = estimator.predict(test)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "## scored  0.56481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
